% This is the aspauthor.tex LaTeX file
% Copyright 2014, Astronomical Society of the Pacific Conference Series
% Revision:  14 August 2014

% To compile, at the command line positioned at this folder, type:
% latex aspauthor
% latex aspauthor
% dvipdfm aspauthor
% This will create a file called aspauthor.pdf.

\documentclass[11pt,twoside]{article}
\usepackage{./asp2014}

\aspSuppressVolSlug
\resetcounters

\bibliographystyle{asp2014}

\markboth{Michael Young and Scott Micheal}{BDBS Big Data Challenges}

\begin{document}

\title{Big Data Challenges in the Blanco DECam Bulge Survey}
\author{Michael Young$^1$ and Scott Michael$^2$
\affil{$^1$Indiana University, Bloomington, IN, USA; 
\email{youngmd@iu.edu}}
\affil{$^2$Indiana University, Bloomington, IN, USA; 
\email{scamicha@iu.edu}}}

% This section is for ADS Processing.  There must be one line per author.
\paperauthor{Michael Young}{youngmd@iu.edu}{}{Indiana University}{Research Technologies}{Bloomington}{IN}{47401}{USA}
\paperauthor{Sample~Author2}{Author2Email@email.edu}{}{Indiana University}{Research Technologies}{Bloomington}{IN}{47401}{USA}

\begin{abstract}
As part of a multi-year effort to survey 200 square degrees of the Southern Milky Way Bulge in SDSS \textit{ugrizY} utilizing the Dark Energy Camera (DECam) on the Blanco 4m telescope, the Blanco DECam Bulge Survey (BDBS) has taken >350k source frames.  Utilizing a distributed architecture executing dophot, we have extracted ~15 billion source detections from these frames.  With one of the primary goals of the survey being the creation of a catalog as a community resource, we have explored a number of ways to facilitate the querying and dissemination of this dataset.  Given the wide and deep nature of the data, a traditional RDBMS is unsuitable for all but the simplest of queries.  Here we will present our efforts to leverage the open-source Apache Hadoop/HDFS/Hive stack, a widely recognized industry-standard approach to Big Data problems.  Running on relatively inexpensive hardware, we will demonstrate how solutions designed for the commercial web have already addressed many of the concerns facing scientists in the Big Data future.
\end{abstract}

\section{Introduction}
Over the past few years there has been a large amount of time and money allocated in the private sector to address the problems and opportunities presented by the ongoing collection of massive amounts of data, commonly known as Big Data.  

\section{Test Setup}

Our test cluster consists of five of the 16 data nodes on the Karst Cluster at IU, which also contains 256 compute nodes not used in our tests.  The data nodes each have 2 Intel Xeon E5-2650 v2 8-core processors, for 16 cores per node and 80 cores in total.  Each node has 64GB of RAM and 24TB of local storage.  

A Apache Hadoop\footnote{\url{https://hadoop.apache.org/}} Distributed File System (HDFS) was installed and configured to utilize 20TB from each node's local storage to construct a distributed and redundant file system with 100TB of available space.  Our Hive tests utilize HDFS, while the LSD and MySQL applications use local storage.  

Our test dataset consists of ~365 million point sources with positions and calibrated $ugrizY$ magnitudes and errors, totaling ~60GB on disk.  This represents a significant fraction of the eventual full BDBS dataset, which we anticipate will total over $10^9$ sources when complete.


\section{Spatial Searches: The power of partitions}
\label{spatial}
Each table within a Hive database can have one or more partition keys, which determine how the data are stored on HDFS.  In addition to determining the file structure, the partition keys act as virtual columns, reducing the storage requirements.  By physically splitting the table by the unique entries of a given key, the speed of a given query may be drastically increased if the query utilizes the partition key as limiting selector, thereby reducing the number of rows which must be iterated over to only those contained within the target partition(s).

Astronomical database queries commonly include a spatial limitation, either selecting objects between some minimum and maximum coordinate range (box search), or in some radius around a given point (cone search).  Of these two, the cone search is the most computationally expensive, requiring the use of algorithms such as the Haversine formula to accurately calculate angular distances on the sky.  Various optimizations have been attempted over the years, but the most successful of these has been sky pixelation techniques, including the HEALPix\footnote{\url{http://healpix.sf.net/}} and Hierarchical Triangular Mesh (HTM)\footnote{\url{http://www.skyserver.org/htm/index.html}} libraries.  Sky pixelation tessellates the sky into regions and sub-regions, with the size and depth of these tessellations dictated by a scaling parameter.  To simplify our cone search we calculate which sky pixel regions our cone intersects, and use the resulting sky pixel values to select which partitions to search on, avoiding a full table scan.

Being familiar with the healpy\footnote{\url{http://healpy.readthedocs.org/}} library, a Python implementation of HEALPix, we chose to construct our partition keys by calulating HEALPix cells for each source position.  In order to determine which scale parameter, or tessellation depth to use in our partitioning scheme we devised a testing procedure.  While there is no theoretical limit to the depth of the tessellation, we have restricted ourselves to relatively large regions as fragmenting a single table into too many partitions would overload the HDFS namenode and reduce performance.  The typical recommendation is to have $<$100 partitions per node, a level we found ourselves at with the HEALPix NSIDE parameter of 32.  NSIDE values are always in powers of 2.  Table \ref{table:partitions} shows how our dataset was partitioned at different NSIDE levels, with each partition level being created in a new table.

\begin{table}[!ht]
\caption{Partitioning the BDBS dataset with HEALPix}
\label{table:partitions}
\smallskip
\begin{center}
{\small
\begin{tabular}{llc}  % l = left, c = centered
\tableline
\noalign{\smallskip}
NSIDE & Partitions & Avg rows per Partition\\
\noalign{\smallskip}
\tableline
\noalign{\smallskip}
2 & 18 & 20259746 \\
4 & 37 & 9856092\\
8 & 57 & 6397814\\
16 & 100 & 3646754\\
32 & 214 & 1704090\\
\noalign{\smallskip}
\tableline\
\end{tabular}
}
\end{center}
\end{table}


Keeping in mind that our goal was to allow efficient querying of our database across a range of search radii, we resolved to test the how different partitions levels performed across a wide parameter space.  We selected random positions within our dataset, to reduce the effect of a single position lying near a vertex at a particular tessellation level.  We also varied the search radius, from 0.1 - 2 degrees.  At each position and radius we checked the performance against all the partition levels, and scaled the runtime to that of the unpartitioned control table.  Finally, we average the scaled runtimes across all positions for a given search radius and tessellation level.   We utilized the Tez engine for this test, with mapping and reduce vectorization enabled, and the data were stored in the ORCFile format, which offers the best performance for the Tez engine.  The results of this test can been in Figure \ref{fig:conesearch}, with scaled runtimes plotted against partition levels, with the different colored lines representing search radii.  

\articlefigure{P121_f2.eps}{fig:conesearch}{Search times vs partition depth.  Search times are scaled to the no-partition control.  HEALPix NSIDE values of 2,4,8,16 and 32 were tested.  Random positions across the entire dataset were sampled, and search radii varied from 0.1 to 2.0 degrees.  Typical searches returned $10^6$ sources.}


Even at the lowest level of NSIDE=2 we see a significant increase in performance, and at our highest partition level (NSIDE=32) we see runtimes on average taking less than 25\% of the unpartitioned runtime.  The performance curve flattens out significantly at the higher levels, indicating that we are running into the limits imposed by the Hive/Tez overhead for our setup. 

\section{Scalability}
With the largest Hadoop clusters in the world containing over 4500 nodes (source?), the question of the scalability of Hadoop has been solved.  The purpose of this test is to find out how adding additional nodes to our small cluster improves performance for our particular dataset.  To accomplish this we created the cluster initially with a single node, ran our tests, then added additional nodes, re-running the same queries as we went along.  We note that the flexibility of Hadoop allows us to add nodes to the cluster without restarting any services.  We ran two types of queries in our test:  a partition restricted cone search as in Section \ref{spatial}, and a whole table color calculation and sort.  

\articlefigure{P121_f3.eps}{fig:scaling}{Runtimes vs number of nodes in the cluster.}

\section{Conclusion and Future Work}

We have successfully demonstrated that solutions developed for Big Data concerns in the commercial sector can be applied to the question of astronomy data management and provide good performance out of the box.  We have demonstrated how we can scale seamlessly to handle large datasets on commodity hardware.  The need for data scientists to develop custom solutions to looming Big Data concerns in the field of astronomy is no longer obvious.  We have also shown that intelligently partitioning our data, spatially and temporally, can have a significant positive impact on query runtimes.  

We note the existence of several relatively new technologies within the SQL-on-Hadoop space, including Impala and SparkSQL, both intended to enable real-time interactive querying, as opposed to the batch-submission approach of Hive.  Implementing and testing these cutting-edge solutions is beyond the immediate scope and requirements of the BDBS project, but neither the promise of these efforts nor the rapid timescale on which they have been developed should be ignored, and bode well for future efforts within this problem space.  

\acknowledgements We'd like to thank our collaborators on the BDBS project, especially Michael Rich, Will Clarkson, and Christian Johnson.  We'd also like to thank the High Performance Servers team within the Research Technology group at Indiana University, for providing the hardware and support for our testing setup. Some of the results in this paper have been derived using the HEALPix (GÃ³rski et al., 2005) package".

%\bibliography{editor}  % For BibTex

% For non-BibTex:
\begin{thebibliography}
\end{thebibliography}

\end{document}
